---
title: "Generate simulated data"
output: html_document
---

**Last updated:** `r Sys.Date()`

**Code version:** `r system("git log -1 --format='%H'", intern = TRUE)`

```{r, echo = FALSE}
knitr::read_chunk("chunks.r")
```

```{r knitr-opts-chunk, include=FALSE}
```

---


## Current considerations

1. SCDE requires at least 2000 genes to run and require filtering...

---

## Simulation strategy

We will compare and evaluate methods based simulated datasets that are generated based on real datasets of single cell RNA-seq and bulk RNA-seq. These simulated datasets will permit us to evaluate the power performance of the methods while controlling for important parameters: number of sequencing samples, number of genes, depth of coverage, fraction of true DE genes. Most importantly, our simulation approach preserves the sample correlation structure in each sequencing datasets and thereby provides "realistic" evaluation datasets.

We will generate three major classes of simulated datasets:  
1. single cell RNA-sequencing datasets, including both UMI and read-based count data  
2. bulk RNA-sequencing data, using GTEx liver samples  
3. thinned bulk RNA-sequencing data  

The simulation parameters are:  
1. Number of sequencing samples/group: n = 40, 80, 200, 500 (the upper limit is fixed for each dataset; expect higher number for drop-seq and at most 90~ for Fluidigm C1)  
2. Number of genes: G = 1,000   
3. Number of simulated datasets: M = 50  
4. Fraction of null genes: $\pi_0 \in \{ .5, .9, 1\}$  
5. Thinning parameter: $\theta \in {.001, .01}$
6. Effect size distribution: $\hat{\beta}_g \sim N(0, 1)$


Here we briefly summarize the steps in simulated data generation:
1. Take a subset of $G$ genes,  
2. Take a subset of $2n$ samples and assign the samples to two arbitrary biological conditions: either same labels at each gene (preserve correlation structure) or different labels at each gene (no sample correlation),  
3. Modify the expression count data to add signal.


## Add signals

The method of adding signals has been previously described: ash (?), vash (?), and Gerard and Stephens (RUV and mouthwash). For the motivation of this approach, see the main text in Gerard and Stephens (RUV) for a detailed description. The overview here closely follows the text in Gerard and Stephens (RUV). 

We add signals to the null datasets in a randomly selected fraction of genes $\pi_1 = 1 - \pi0$ ($\pi_1 = .1$ or $.5$). The signals $\beta_g$ are randomly sampled from a $N(0, ?)$. the standard deviation was chosen so that the classification would be neither too easy nor too hard (?). Let 

$\beta_{g1}, \dots, \beta_{g \pi_1*p} \sim N(0, s^2)$

be the effect sizes, where $gl \in \Omega$, the set of non-null genes. Then we drew a $\mathbf{Z}$ matrix of the same dimension as the expression count matrix $\mathbf{Y}$ by 

\[
z_{igl} | y_{igl} \sim 
\begin{cases}
\text{Binomial}(y_{igl}, 2^{\beta_{gl} x_{i}}) & \text{if $\beta_{gl} < 0$ and $gl \in \Omega$},\\
\text{Binomial}(y_{igl}, 2^{-\beta_{gl} (1-x_{i})}) & \text{if $\beta_{gl} > 0$ and $gl \in \Omega$},\\
\delta(y_{igl}) & \text{if $jl \notin \Omega$}, 
\end{cases}
\]

where $\delta(y_{igl})$ indicates a point mass at $y_{igl}$. We then use $\mathbf{Z}$ as our non-null expression count matrix.



### How to choose $s^2$?

As a rule of thumb, large $s^2$ generates large effect sizes and thus DE classification is easier. To help choosing the value of $s^2$, I compute the fraction of true positive genes (fraction of true signals classifed as DE) given 5% and 1% false positive rate (proportion of nulls classifed as DE) under the parameter setting:

1. Number of sequencing samples per group: 50 and 100  
2. Number of genes: 1000  
3. Number of simulated data: 50
4. Fraction of null genes: .5, .9

`limmaVoom` was used to perform DE analysis. Overall, a stringent FDR threshold makes classification more difficult (lower TPR compared when 1% FDR compared to 5% FDR) under different sample size, fraction of null genes, and $s^2$. About the parameter settings, all but fraction of null genes may affect TPR under fixed FDR. Larger sample size is associated with higher TPR. More importantly, a large $s^2$ is associated with high TPR - which is as expected since there are more large effect sizes.

When choosing $s^2$, the rule of thumb is setting a value of false positve rate that makes the classification neither easy nor difficult. We chose a value of $s^2$ such taht TPR is about 50%. (maybe report results under other $s^2$ values in the supplemental materials). For now, we choose 1% FDR and $s = 1$.

```{r betasd-and-tpr, eval = FALSE, echo = FALSE}
#' @title Wrapper for simulating datasets 
#' @param counts gene by sample count matrix
#' @param Nsim number of simulated datasets
#' @param Nsample number of samples per biological condition
#' @param Ngene number of genes. Defaults to include all genes in the input data.
simulationWrapper <- function(counts, 
                              Nsim, Nsample, betasd, Ngene = NULL) {

  counts_bignormal_5 <- lapply(1:Nsim, function(i) {
  #  set.seed(999*i)
    foo <- ashbun::makeSimCount2groups(counts = counts,
                               Nsamp = Nsample, Ngene = Ngene,
                               sample_method = "all_genes")
    foo2 <- ashbun::non_null_sim(counts = foo$counts, 
                         args = args.big_normal(nsam = Nsample, 
                                                betasd = betasd, pi0 = .5))
    
    geneToInclude <- which(rowSums(foo2$counts) != 0)
    sampleToInclude <- which(colSums(foo2$counts) != 0)
    foo2$counts <- foo2$counts[geneToInclude, sampleToInclude]
    foo2$condition <- foo2$condition[sampleToInclude]
    foo2$null <- foo2$null[geneToInclude]
    return(foo2)
  })

    counts_bignormal_9 <- lapply(1:Nsim, function(i) {
  #  set.seed(999*i)
    foo <- ashbun::makeSimCount2groups(counts = counts,
                               Nsamp = Nsample, Ngene = Ngene,
                               sample_method = "all_genes")
    foo2 <- ashbun::non_null_sim(counts = foo$counts, 
                         args = args.big_normal(nsam = Nsample, 
                                                betasd = betasd, pi0 = .9))
    
    geneToInclude <- which(rowSums(foo2$counts) != 0)
    sampleToInclude <- which(colSums(foo2$counts) != 0)
    foo2$counts <- foo2$counts[geneToInclude, sampleToInclude]
    foo2$condition <- foo2$condition[sampleToInclude]
    foo2$null <- foo2$null[geneToInclude]
    return(foo2)
  })

  return(list(counts_bignormal_5 = counts_bignormal_5,
              counts_bignormal_9 = counts_bignormal_9))
}

#<-------------------------------
# Generate simulated data

library(ashbun)
library(singleCellRNASeqHumanTungiPSC)
data <- get(data("HumanTungiPSC"))
data_sub <- exprs(data)[,pData(data)$individual == "NA19239"]


betasd_vector <- c(1, 4)
simdata_n50 <- vector("list", length(betasd_vector))
simdata_n100 <- vector("list", length(betasd_vector))
names(simdata_n100) <-names(simdata_n50) <- paste0("betasd_",betasd_vector)
Nsim <- 50

for (index in 1:length(simdata_n50)) {
  simdata_n50[[index]] <- simulationWrapper(data_sub, Nsim = Nsim, 
                                             Nsample = 50,
                                             betasd = betasd_vector[[index]], 
                                             Ngene = 1000)
}

for (index in 1:length(simdata_n100)) {
  simdata_n100[[index]] <- simulationWrapper(data_sub, Nsim = Nsim, 
                                             Nsample = 100,
                                             betasd = betasd_vector[[index]], 
                                             Ngene = 1000)
}



#<-------------------------------
# Apply limmaVoom

fit_n50 <- vector("list", length(betasd_vector))
fit_n100 <- vector("list", length(betasd_vector))
names(fit_n100) <-names(fit_n50) <- paste0("betasd_",betasd_vector)

fdr_vector <- c(.1, .5)
tpr01_n50 <- tpr05_n50 <- vector("list", length(betasd_vector))
tpr01_n100 <- tpr05_n100 <- vector("list", length(betasd_vector))
names(tpr01_n50) <- names(tpr05_n50) <- tpr01_n100 <- tpr05_n100 <- paste0("betasd_",betasd_vector)

# for each FDR cutoff
for (index.fdr in 1:length(fdr_vector)) {
  fdr_cutoff <- fdr_vector[index.fdr]
  cat("fdr_cutoff", fdr_cutoff, "\n")
# for each sample size
for (index in 1:2) {
  
  if (index == 1) {
    simdata_list <- simdata_n50 
    #fit_list <- fit_n50; tpr_list <- tpr_n50
    cat("n=50", "\n") 
  }
  if (index == 2) {
    simdata_list <- simdata_n100
    #fit_list <- fit_n100; tpr_list <- tpr_n100
    cat("n=100", "\n")
  }
  
# for each betasd
  for (index.0 in 1:length(simdata_list)) {
    
    fit_list[[index.0]] <- tpr_list[[index.0]] <- vector("list", length(simdata_list[[index.0]]))
    names(fit_list[[index.0]]) <- names(tpr_list[[index.0]]) <- names(simdata_list[[index.0]])
  
    cat(names(fit_list)[[index.0]], "\n")  
    
  # for each pi0  
    for (index.1 in 1:length(simdata_list[[index.0]])) {
      fit_list[[index.0]][[index.1]] <- tpr_list[[index.0]][[index.1]] <- vector("list", length(simdata_list[[index.0]][[index.1]]))
    
    cat(names(fit_list[[index.0]])[[index.1]], "\n")  
  
  # for each simulated data    
      for (index.2 in 1:length(simdata_list[[index.0]][[index.1]])) {
    
        cat("No. sim", index.2, "\n")  
  
        obj <- simdata_list[[index.0]][[index.1]][[index.2]]
        fit_list[[index.0]][[index.1]][[index.2]] <- ashbun::methodWrapper.limmaVoom(obj$counts,
                                                                                    obj$condition)
        tpr_list[[index.0]][[index.1]][[index.2]] <- 
          ashbun::getTPR(response = !obj$nul,
                         predictor = fit_list[[index.0]][[index.1]][[index.2]]$p.value[,2],
                         fdr_cutoff = fdr_cutoff)
      }
    }
  }
  if (index.fdr == 1) {
    if (index == 1) { fit_n50 <- fit_list; tpr01_n50 <- tpr_list}
    if (index == 2) { fit_n100 <- fit_list; tpr01_n100 <- tpr_list}
  }
  if (index.fdr == 2) {
    if (index == 1) { fit_n50 <- fit_list; tpr05_n50 <- tpr_list}
    if (index == 2) { fit_n100 <- fit_list; tpr05_n100 <- tpr_list}
  }
  }
}

#<-----------------------------
# organize results for plotting
make_tpr_short_to_long <- function(tpr_list) {
  tpr_long <- do.call(rbind, lapply(1:length(tpr_list), function(index) {
      per_pi0 <- lapply(1:length(tpr_list[[index]]), function(index.0) {
                  per_sim <- data.frame(tpr = do.call("c", tpr_list[[index]][[index.0]]),
                                        Nsim = c(1:length(tpr_list[[index]][[index.0]])))
                  return(per_sim) 
      }) 
    #  names(per_pi0) <- names(tpr_list[[index]])
      per_pi0 <- do.call(rbind, per_pi0)
      per_pi0$pi0 <- rep(c(".5", ".9"), each = length(tpr_list[[index]][[index.0]]))
      per_pi0$betasd <- betasd_vector[index]
      
      return(per_pi0)
  })  )
  return(tpr_long)
}

tpr01_n50_long <- make_tpr_short_to_long(tpr01_n50)
tpr01_n100_long <- make_tpr_short_to_long(tpr01_n100)
tpr05_n50_long <- make_tpr_short_to_long(tpr05_n50)
tpr05_n100_long <- make_tpr_short_to_long(tpr05_n100)

tpr01_n50_long$Nsam <- tpr05_n50_long$Nsam <- "50"
tpr01_n100_long$Nsam <- tpr05_n100_long$Nsam <- "100"

tpr01_n50_long$fdr <- tpr01_n100_long$fdr <- ".01"
tpr05_n50_long$fdr <- tpr05_n100_long$fdr <- ".05"


tpr01_long <- rbind(tpr01_n50_long, tpr01_n100_long)
tpr05_long <- rbind(tpr05_n50_long, tpr05_n100_long)

save(tpr01_long, tpr05_long, file = "../output/benchmark-simulation.Rmd/tpr_long.rda")
```


```{r plot-tpr, eval = TRUE, echo = FALSE}
#<-----------------------------
# plotting results for plotting
load("../output/benchmark-simulation.Rmd/tpr_long.rda")
make_tpr_plot <- function(tpr_long, title) {
  library(ggplot2)
  library(dplyr)
  tpr_long_summary <- tpr_long %>% 
                        group_by(Nsam, betasd, pi0) %>% 
                        summarise(med = median(tpr))
  print(
    ggplot(data = tpr_long,
       mapping = aes(x = pi0, y = tpr)) +
    geom_point(cex = .6) +
    theme_bw() + ggtitle(title) +
    facet_wrap( ~ Nsam + betasd) +
    geom_point(data = tpr_long_summary, 
                aes(x = pi0, y = med), 
                size = 2, col = "red") +
    ylab("True Postive Rate") )
}
```


```{r}
make_tpr_plot(tpr01_long, title = "TPR given 1% FDR")
```

```{r}
make_tpr_plot(tpr05_long, title = "TPR given 5% FDR")
```


### How to choose parameter settings for different distributional assumptions?


question: what parameter values are required for shape A and shape B to have the same TPR under fixed FDR, sample size, and pi0



---

## Other papers

In Soneson and Robinson (2017), the simulated datasets include a subset of single cell samples but all of the genes in the original datasets.




---

## Session information

```{r, echo = FALSE}
sessionInfo()
```




